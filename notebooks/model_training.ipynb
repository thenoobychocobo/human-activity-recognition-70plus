{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "**Table of Contents**\n",
    "1. [Setup](#1-setup)\n",
    "2. [Dataset Preparation](#2-dataset-preparation)\n",
    "3. [Training Loop](#3-training-loop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Required Modules #\n",
    "####################\n",
    "\n",
    "# Generic/Built-in\n",
    "import random\n",
    "import sys \n",
    "import os\n",
    "\n",
    "# Libs\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project root directory to the system path to enable imports from the '/src' folder.\n",
    "\n",
    "# Get the project directory \n",
    "current_dir = os.path.abspath('') # Current '\\notebooks' directory\n",
    "project_dir = os.path.abspath(os.path.join(current_dir, '..')) # Move up one level to project root directory\n",
    "\n",
    "# Add the project directory to sys.path\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "# Move up to project directory\n",
    "os.chdir(project_dir)\n",
    "os.getcwd()\n",
    "\n",
    "# Import custom modules\n",
    "from src.data_preparation import *\n",
    "from src.models import *\n",
    "from src.train_eval import *\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding\n",
    "SEED = 42\n",
    "\n",
    "# To be safe, seed all modules for full reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # If using CUDA\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Hyperparameters\n",
    "sequence_size = 250 \n",
    "stride = 125\n",
    "num_train = 32\n",
    "num_val = 4\n",
    "num_test = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = prepare_datasets(\n",
    "    sequence_size=sequence_size, \n",
    "    stride=stride,\n",
    "    num_train=num_train,\n",
    "    num_val=num_val,\n",
    "    num_test=num_test,\n",
    "    random_state=SEED, # For reproducibility\n",
    "    load_if_exists=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train Set: {len(train_dataset)} samples\")\n",
    "print(f\"Validation Set: {len(val_dataset)} samples\")\n",
    "print(f\"Test Set: {len(test_dataset)} samples\")\n",
    "print(f\"Total: {len(train_dataset) + len(val_dataset) + len(train_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "learning_rate = 0.001\n",
    "num_epochs = 40\n",
    "weight_decay = 1e-5 # L2 Regularization coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model in the cell below. There are 4 model architectures to choose from: `HarLSTM`, `HarGRU`, `HarTransformer`, and `HarTransformerExperimental`. The default parameter values should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_kwargs = {}\n",
    "model = HarTransformerExperimental(**model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr = learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Continue training by loading trained parameters\n",
    "trained_params_path = \"models/HarTransformerExperimental_2025-04-15_16-28-09/HarTransformerExperimental_best_F1.pth\" # Specify path to .pth file here\n",
    "# model.load_state_dict(torch.load(trained_params_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_history, validation_loss_history, micro_accuracy_history, macro_accuracy_history, f1_history, precision_history, recall_history, normalizer = train_HAR70_model(\n",
    "    model, \n",
    "    optimizer, \n",
    "    train_dataloader, \n",
    "    validation_dataloader, \n",
    "    num_epochs = num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = save_training_plots_and_metric_history(\n",
    "    training_loss_history, validation_loss_history, micro_accuracy_history, macro_accuracy_history,\n",
    "    f1_history, precision_history, recall_history, type(model).__name__\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "loss, micro_accuracy, macro_accuracy, f1, precision, recall, conf_matrix = evaluate_HAR70_model(model, test_dataloader, normalizer)\n",
    "print(f\"(Test) Loss: {loss:.4f}, Accuracy (micro): {micro_accuracy:.4f}, Accuracy (macro): {macro_accuracy:.4f}, F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics_from_confusion_matrix(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original label mapping (12)\n",
    "class_names = [\n",
    "    \"Walking\", # Label 0\n",
    "    \"Running\",\n",
    "    \"Shuffling\",\n",
    "    \"Stairs (ascending)\",\n",
    "    \"Stairs (descending)\",\n",
    "    \"Standing\",\n",
    "    \"Sitting\",\n",
    "    \"Lying\",\n",
    "    \"Cycling (sit)\",\n",
    "    \"Cycling (stand)\",\n",
    "    \"Cycling (sit, inactive)\",\n",
    "    \"Cycling (stand, inactive)\" # Label 11\n",
    "]\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_name = \"unnormalized_conf_matrix\"\n",
    "plot_and_save_confusion_matrix(\n",
    "    save_dir=save_dir,\n",
    "    conf_matrix=conf_matrix,\n",
    "    file_name=plot_name,\n",
    "    class_names=class_names\n",
    ")\n",
    "metric_results = save_and_compute_metrics_from_confusion_matrix(\n",
    "    save_dir=save_dir,\n",
    "    conf_matrix=conf_matrix,\n",
    "    file_name=plot_name\n",
    ")\n",
    "metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize confusion matrix (row-wise)\n",
    "conf_matrix_normalized = normalize_confusion_matrix(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_name = \"normalized_conf_matrix\"\n",
    "plot_and_save_confusion_matrix(\n",
    "    save_dir=save_dir,\n",
    "    conf_matrix=conf_matrix_normalized,\n",
    "    file_name=plot_name,\n",
    "    class_names=class_names\n",
    ")\n",
    "metric_results = save_and_compute_metrics_from_confusion_matrix(\n",
    "    save_dir=save_dir,\n",
    "    conf_matrix=conf_matrix_normalized,\n",
    "    file_name=plot_name\n",
    ")\n",
    "metric_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge and ignore classes for a fairer comparison with HAR70+ paper results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_names)\n",
    "## WIP WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_conf_matrix, updated_class_names = ignore_classes(conf_matrix, class_names, [1,8,9,10,11])\n",
    "updated_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_groups = [\n",
    "    [0, 2, 3], # Walking + Stairs (ascending) + Stairs (descending)\n",
    "    [1, 4], # Shuffling + Standing\n",
    "]\n",
    "updated_conf_matrix, updated_class_names = merge_multiple_classes(\n",
    "    conf_matrix=updated_conf_matrix,\n",
    "    merge_groups=merge_groups,\n",
    "    merge_names=[\"Walking (merged)\", \"Standing (merged)\"],\n",
    "    class_names=updated_class_names\n",
    ")\n",
    "updated_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize confusion matrix (row-wise)\n",
    "updated_conf_matrix_normalized = normalize_confusion_matrix(updated_conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_and_save_confusion_matrix(\n",
    "    save_dir=save_dir,\n",
    "    conf_matrix=updated_conf_matrix_normalized,\n",
    "    file_name=\"merged_conf_matrix_normalized\",\n",
    "    class_names=updated_class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics_from_confusion_matrix(updated_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save session information\n",
    "save_model_information(\n",
    "    # Dataset Config:\n",
    "    save_dir=save_dir, \n",
    "    sequence_size=sequence_size, \n",
    "    stride=stride, \n",
    "    num_train=num_train, \n",
    "    num_val=num_val, \n",
    "    num_test=num_test, \n",
    "    random_state=SEED,\n",
    "    # Training Hyperparams:\n",
    "    optimizer_name=type(optimizer).__name__,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    num_epochs=num_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    # Model Hyperparams:\n",
    "    model_kwargs=model_kwargs,\n",
    "    # Test Results:\n",
    "    loss=loss,\n",
    "    micro_accuracy=micro_accuracy,\n",
    "    macro_accuracy=macro_accuracy,\n",
    "    f1=f1,\n",
    "    precision=precision,\n",
    "    recall=recall\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "term6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
